# Runs Router - Handles agent run execution, listing, and management
import asyncio
import json
from pathlib import Path
from datetime import datetime
from fastapi import APIRouter, BackgroundTasks, HTTPException, Request, Body
from pydantic import BaseModel
from typing import Optional

from shared.state import (
    active_loops,
    get_multi_mcp,
    get_remme_store,
    get_remme_extractor,
    PROJECT_ROOT,
)
from core.loop import AgentLoop4
from core.graph_adapter import nx_to_reactflow
from remme.utils import get_embedding
from config.settings_loader import settings

router = APIRouter(tags=["Runs"])

# Get shared instances
multi_mcp = get_multi_mcp()
remme_store = get_remme_store()
remme_extractor = get_remme_extractor()


# === Pydantic Models ===

class RunRequest(BaseModel):
    query: str
    model: str = None  # Will use settings default if not provided
    
    def __init__(self, **data):
        super().__init__(**data)
        if self.model is None:
            self.model = settings.get("agent", {}).get("default_model", "gemini-2.5-flash")


class RunResponse(BaseModel):
    id: str
    status: str
    created_at: str
    query: str


class UserInputRequest(BaseModel):
    node_id: str
    response: str


class AgentTestRequest(BaseModel):
    """Optional request body for agent testing"""
    pass


# === Background Tasks ===

async def process_run(run_id: str, query: str):
    """Background task to execute the agent loop"""
    try:
        # 1. RETRIEVE MEMORIES (Remme)
        # Search for past relevant facts to injecting into this run
        memory_context = ""
        context = None # Initialize for safe access in finally block
        results = []
        try:
            emb = get_embedding(query, task_type="search_query")
            results = remme_store.search(emb, query_text=query, k=3)
            if results:
                memory_str = "\n".join([f"- {r['text']} (Confidence: {r.get('score', 0):.2f})" for r in results])
                memory_context = f"PREVIOUS MEMORIES ABOUT USER:\n{memory_str}\n"
                print(f" Remme: Injected {len(results)} memories into run {run_id}")
        except Exception as e:
            print(f"‚ö†Ô∏è Remme Retrieval Failed: {e}")

        loop = AgentLoop4(multi_mcp=multi_mcp)
        # Register the LOOP instance immediately so we can stop it
        active_loops[run_id] = loop
        
        # Execute the loop
        # The loop will maintain its own internal context
        print(f"[{run_id}] MEMORY CONTEXT INJECTED:\n{memory_context}")
        try:
             context = await loop.run(query, [], {}, [], session_id=run_id, memory_context=memory_context)
        except asyncio.CancelledError:
             print(f"[{run_id}] Run cancelled.")
             context = loop.context # Recovery context from loop if possible
        
        # 2. EXTRACT NEW MEMORIES (Remme)
        # We put this in a finally block? No, because we want it only on success/completion of meaningful work.
        # But if user stops it, we might want to extract partials.
        # For now, let's leave it after run() but handle the stop case explicitly if context is returned.
        
    except Exception as e:
        print(f"Run {run_id} failed: {e}")
    finally:
        # Clean up
        if run_id in active_loops:
            del active_loops[run_id]
            
        # Attempt extraction if we have context (even if stopped)
        # Note: 'context' variable needs to be accessible here.
        pass 
        # After run completes, extract new facts
        try:
            # Get the history from context (Plan Graph or Session Summary)
            # For now, we don't return the full conversation history from loop.run directly
            # But context has plan_graph... 
            # Ideally we extract from the "Summary" generated by the ReportingAgent if available
            # OR we can pass the query and the FINAL output.
            
            # Simple V1: Extract from Query + Final Answer (if available)
            final_output = ""
            # Try to find final output from graph
            if context and context.plan_graph:
                # Find nodes with output
                for node_id in context.plan_graph.nodes:
                    node = context.plan_graph.nodes[node_id]
                    if node.get("status") == "completed" and node.get("output"):
                        final_output += f"{node_id} Output: {str(node['output'])}\n"

            history = [{"role": "assistant", "content": final_output}]
            
            print(f" Remme: Extracting facts from run {run_id}...")
            # Pass existing memories from earlier search to context-aware extractor
            # ‚ö° RUN IN THREAD TO AVOID BLOCKING EVENT LOOP
            commands, preferences = await asyncio.to_thread(
                remme_extractor.extract, 
                query, 
                history, 
                existing_memories=results
            )
            
            if commands:
                for cmd in commands:
                    if not isinstance(cmd, dict):
                        print(f"‚ö†Ô∏è Remme: Skipping invalid command format: {cmd}")
                        continue
                        
                    action = cmd.get("action")
                    text = cmd.get("text")
                    target_id = cmd.get("id")
                    
                    try:
                        if action == "add" and text:
                            emb = get_embedding(text, task_type="search_document")
                            remme_store.add(text, emb, category="derived", source=f"run_{run_id}")
                            print(f"‚úÖ Remme: Added new fact: {text}")
                        elif action == "update" and target_id and text:
                            emb = get_embedding(text, task_type="search_document")
                            remme_store.update_text(target_id, text, emb)
                            print(f"üîÑ Remme: Updated fact {target_id}: {text}")
                        elif action == "delete" and target_id:
                            remme_store.delete(target_id)
                            print(f"üóëÔ∏è Remme: Deleted fact {target_id}")
                    except Exception as e:
                        print(f"‚ùå Remme Action Failed: {e}")
            
            # Apply preferences to hubs
            if preferences:
                from remme.extractor import apply_preferences_to_hubs
                apply_preferences_to_hubs(preferences)
                print(f"‚úÖ Remme: Processed {len(preferences)} preference updates.")
                
                print(f"‚úÖ Remme: Processed {len(commands)} memory updates.")
            else:
                 print(f"‚ÑπÔ∏è Remme: No new facts extracted from run {run_id}.")

        except Exception as e:
            print(f"‚ö†Ô∏è Remme Extraction Failed: {e}")
            import traceback
            traceback.print_exc()

        # 3. AUTO-SAVE REPORTS TO NOTES
            # 3. AUTO-SAVE REPORTS TO NOTES
        try:
             if context and context.plan_graph:
                import re
                
                notes_dir = PROJECT_ROOT / "data" / "Notes" / "Arcturus"
                notes_dir.mkdir(parents=True, exist_ok=True)
                
                def sanitize_filename(title):
                    title = re.sub(r'[\\/*?:"<>|#]', "", title)
                    title = title.replace("\n", " ").strip()
                    return title[:60].strip()

                def extract_title(content):
                    match = re.search(r'^#+\s+(.+)$', content, re.MULTILINE)
                    if match:
                        return match.group(1).strip()
                    # Fallback to first non-empty line
                    lines = [l.strip() for l in content.split('\n') if l.strip()]
                    if lines:
                        return lines[0]
                    return "Untitled Report"

                for node_id in context.plan_graph.nodes:
                    node = context.plan_graph.nodes[node_id]
                    agent_type = node.get("agent", "")
                    output = node.get("output", {})
                    if not output: continue

                    # Check for Formatter output keys
                    markdown = output.get("markdown_report")
                    if not markdown:
                         for k, v in output.items():
                             if k.startswith("formatted_report") and isinstance(v, str):
                                 markdown = v
                                 break
                    
                    if markdown and len(markdown) > 100:
                         title = extract_title(markdown)
                         filename = sanitize_filename(title) + ".md"
                         target_path = notes_dir / filename
                         
                         if target_path.exists() and len(target_path.read_text(encoding='utf-8')) >= len(markdown):
                             continue

                         with open(target_path, 'w', encoding='utf-8') as f:
                             f.write(markdown)
                         print(f"‚úÖ Auto-Saved Report to Notes: {filename}")

        except Exception as e:
            print(f"‚ö†Ô∏è Failed to auto-save report: {e}")
            
        # Return result for Scheduler/Skills
        final_result = {"status": "completed", "run_id": run_id}
        if context and context.plan_graph:
            # Check for any failed nodes
            for node_id in context.plan_graph.nodes:
                node = context.plan_graph.nodes[node_id]
                if node.get("status") == "failed":
                    final_result["status"] = "failed"
                    final_result["error"] = node.get("error")
                    break
        
        if context:
             try:
                 output_str = ""
                 if context.plan_graph:
                     # 1. Look for FormatterAgent output first (The Final Report)
                     for node_id in context.plan_graph.nodes:
                         node = context.plan_graph.nodes[node_id]
                         node_agent = node.get("agent", "")
                         out = node.get("output", {})
                         
                         if node_agent == "FormatterAgent" or "Format" in node_agent:
                             if isinstance(out, dict):
                                 md = out.get("markdown_report")
                                 if not md:
                                     # Try all keys for something that looks like a report
                                     for k, v in out.items():
                                         if (k.startswith("formatted_report") or k == "report") and isinstance(v, str):
                                             md = v
                                             break
                                 
                                 if md:
                                     output_str = md
                                     break
                                 
                                 # Fallback: if 'output' is a long string
                                 if isinstance(out.get("output"), str) and len(out["output"]) > 100:
                                     output_str = out["output"]
                                     break
                     
                     # 2. Fallback: Find any node with a substantial string output
                     if not output_str:
                         for node_id in reversed(list(context.plan_graph.nodes)):
                             if node_id == "ROOT": continue
                             node = context.plan_graph.nodes[node_id]
                             out = node.get("output", {})
                             
                             if isinstance(out, dict):
                                 # Try to find the largest string value in the dict (recursive search)
                                 def find_largest_string(d):
                                     largest = ""
                                     for v in d.values():
                                         if isinstance(v, str):
                                             if len(v) > len(largest):
                                                 largest = v
                                         elif isinstance(v, dict):
                                             sub = find_largest_string(v)
                                             if len(sub) > len(largest):
                                                 largest = sub
                                     return largest
                                 
                                 largest_str = find_largest_string(out)
                                 if len(largest_str) > 50:
                                     output_str = largest_str
                                     break
                                     
                             elif isinstance(out, str) and len(out) > 50:
                                 output_str = out
                                 break

                 # 3. RUTHLESS CLEANING: Remove typical JSON leakage if content is actually Markdown
                 if output_str:
                     import re
                     # If the output starts and ends with {} or [], it might be a JSON dump 
                     # that contains a markdown_report field.
                     if (output_str.startswith("{") and output_str.endswith("}")) or (output_str.startswith("[") and output_str.endswith("]")):
                         try:
                             # Try to parse it and extract the report field
                             data = json.loads(output_str)
                             if isinstance(data, dict):
                                 # Look for report-like keys
                                 for k in ["markdown_report", "formatted_report", "output", "summary", "report"]:
                                     if data.get(k) and isinstance(data[k], str) and len(data[k]) > 50:
                                         output_str = data[k]
                                         break
                         except:
                             pass
                     
                     # Final check: remove block delimiters if LLM wrapped them in ```markdown
                     output_str = re.sub(r'^```(?:markdown)?\n', '', output_str)
                     output_str = re.sub(r'\n```$', '', output_str)

                 final_result["output"] = output_str.strip() if output_str else "No substantial output found."
                 if final_result.get("status") == "failed":
                     final_result["summary"] = f"Failed: {final_result.get('error', 'Unknown error')}"
                 else:
                     final_result["summary"] = output_str.strip() if output_str else "Completed."
             except Exception as e:
                 print(f"‚ö†Ô∏è Extraction Error: {e}")
        
        return final_result


# === Endpoints ===

@router.post("/runs")
async def create_run(request: RunRequest, background_tasks: BackgroundTasks):
    run_id = str(int(datetime.now().timestamp()))
    
    # Start background execution
    background_tasks.add_task(process_run, run_id, request.query)
    
    return {
        "id": run_id,
        "status": "starting",
        "created_at": datetime.now().isoformat(),
        "query": request.query
    }


@router.get("/runs")
async def list_runs():
    """List runs from disk"""
    summaries_dir = PROJECT_ROOT / "memory" / "session_summaries_index"
    runs = []
    
    if summaries_dir.exists():
        # Walk through date folders
        for date_folder in summaries_dir.glob("*/*/*"):
            for session_file in date_folder.glob("session_*.json"):
                try:
                    data = json.loads(session_file.read_text())
                    graph_data = data
                    # Extract meta
                    graph_details = graph_data.get("graph", {})
                    
                    # Robust Query Extraction
                    query = graph_details.get("original_query")
                    if not query:
                        query = graph_details.get("globals", {}).get("original_query", "Unknown Query")

                    # Timestamp Extraction
                    created_at = graph_details.get("created_at")
                    if not created_at:
                        # Fallback to file creation time
                        created_at = datetime.fromtimestamp(session_file.stat().st_ctime).isoformat()
                    
                    # Compute status from node statuses
                    # Check nodes for their statuses
                    nodes = data.get("nodes", [])
                    node_statuses = [n.get("status", "pending") for n in nodes if n.get("id") != "ROOT"]
                    
                    if any(s == "running" for s in node_statuses):
                        computed_status = "running"
                    elif any(s == "failed" for s in node_statuses):
                        computed_status = "failed" 
                    elif all(s == "completed" for s in node_statuses) and node_statuses:
                        computed_status = "completed"
                    else:
                        # Fallback to graph-level status or completed
                        computed_status = graph_details.get("status", "completed")
                    
                    total_tokens = sum(
                        (n.get("total_tokens", 0) or 0) for n in nodes
                    )
                    
                    runs.append({
                        "id": session_file.stem.replace("session_", ""),
                        "query": query, 
                        "created_at": created_at, 
                        "status": computed_status,
                        "total_tokens": total_tokens
                    })
                except:
                    continue
    
    # Sort by recent
    return sorted(runs, key=lambda x: x['id'], reverse=True)


@router.get("/runs/{run_id}")
async def get_run(run_id: str):
    """Get graph state for a run"""
    # Check memory first (if running)
    # Then check disk
    
    # Search disk
    summaries_dir = PROJECT_ROOT / "memory" / "session_summaries_index"
    found_file = None
    
    # Brute force search (should optimize path structure later)
    for path in summaries_dir.rglob(f"session_{run_id}.json"):
        found_file = path
        break
        
    if found_file:
        data = json.loads(found_file.read_text())
        # Reconstruct Graph to use adapter
        import networkx as nx
        if "edges" in data:
            G = nx.node_link_graph(data, edges="edges")
        elif "links" in data:
            G = nx.node_link_graph(data, edges="links")
        elif "link" in data:
            # networkx default uses 'link' (singular)
            G = nx.node_link_graph(data, edges="link")
        else:
            data["edges"] = []
            G = nx.node_link_graph(data, edges="edges")
        react_flow = nx_to_reactflow(G)
        
        # Determine status: Running if in memory, else use file status
        status = "running" if run_id in active_loops else data.get("graph", {}).get("status", "completed")

        return {
            "id": run_id,
            "status": status,
            "graph": react_flow
        }
        
    raise HTTPException(status_code=404, detail="Run not found")


@router.post("/runs/{run_id}/input")
async def provide_input(run_id: str, request: UserInputRequest):
    """Provide user input to a running agent (e.g., ClarificationAgent response)"""
    if run_id in active_loops:
        loop = active_loops[run_id]
        if loop.context:
            try:
                # Find the ClarificationAgent node that's awaiting input
                for node_id, node_data in loop.context.plan_graph.nodes(data=True):
                    if node_data.get("agent") == "ClarificationAgent" and node_data.get("status") in ["running", "waiting_input"]:
                        # Get the writes key for this clarification
                        writes = node_data.get("writes", ["user_clarification"])
                        write_key = writes[0] if writes else "user_clarification"
                        
                        # Store user input in globals_schema
                        loop.context.plan_graph.graph.setdefault("globals_schema", {})[write_key] = request.response
                        
                        # Mark the clarification step as completed with user's response as output
                        loop.context.plan_graph.nodes[node_id]["output"] = {
                            "clarificationMessage": "User provided clarification",
                            write_key: request.response
                        }
                        loop.context.plan_graph.nodes[node_id]["status"] = "completed"
                        
                        # Save the session
                        loop.context._save_session()
                        
                        return {"id": run_id, "status": "input_received", "stored_as": write_key}
                
                # No running ClarificationAgent found
                raise HTTPException(status_code=400, detail="No ClarificationAgent is currently waiting for input")
            except HTTPException:
                raise
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"Error processing input: {str(e)}")
        else:
            raise HTTPException(status_code=400, detail="Context not initialized")
    
    raise HTTPException(status_code=404, detail="Active run not found")


@router.post("/runs/{run_id}/stop")
async def stop_run(run_id: str):
    """Stop a running agent execution"""
    if run_id in active_loops:
        loop = active_loops[run_id]
        loop.stop()
        return {"id": run_id, "status": "stopping"}
    
    raise HTTPException(status_code=404, detail="Active run not found")


@router.delete("/runs/{run_id}")
async def delete_run(run_id: str):
    """Delete a run from disk and memory"""
    # 1. Stop if running
    if run_id in active_loops:
        loop = active_loops[run_id]
        loop.stop()
        del active_loops[run_id]
        
    # 2. Delete file
    summaries_dir = PROJECT_ROOT / "memory" / "session_summaries_index"
    deleted = False
    
    # Brute force search
    for path in summaries_dir.rglob(f"session_{run_id}.json"):
        try:
            path.unlink()
            deleted = True
            break
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to delete file: {str(e)}")
            
    if not deleted and run_id not in active_loops: # If wasn't running and file not found
        # Might be okay if it was just in memory? But we are memory-less persistence mostly
        # Let's return success if we stopped it at least, or warn
        pass

    return {"id": run_id, "status": "deleted"}


# === AGENT TESTING ENDPOINTS ===

class AgentTestRequest(BaseModel):
    input: Optional[str] = None

@router.post("/runs/{run_id}/agent/{node_id}/test")
async def test_agent(run_id: str, node_id: str, request: AgentTestRequest = Body(None)):
    """
    Re-run a single agent in TEST MODE (sandbox).
    - Loads the session
    - Extracts the node's inputs from globals_schema
    - Runs the agent with those inputs
    - Returns the NEW output WITHOUT saving to session
    """
    try:
        # 1. Find the session file
        summaries_dir = PROJECT_ROOT / "memory" / "session_summaries_index"
        found_file = None
        for path in summaries_dir.rglob(f"session_{run_id}.json"):
            found_file = path
            break
        
        if not found_file:
            raise HTTPException(status_code=404, detail="Session not found")
        
        # 2. Load session data
        import networkx as nx
        session_data = json.loads(found_file.read_text())
        if "edges" in session_data:
            G = nx.node_link_graph(session_data, edges="edges")
        elif "links" in session_data:
            G = nx.node_link_graph(session_data, edges="links")
        elif "link" in session_data:
            G = nx.node_link_graph(session_data, edges="link")
        else:
            session_data["edges"] = []
            G = nx.node_link_graph(session_data, edges="edges")
        
        # 3. Find the node
        if node_id not in G.nodes:
            raise HTTPException(status_code=404, detail=f"Node {node_id} not found in session")
        
        node_data = G.nodes[node_id]
        agent_type = node_data.get("agent")
        
        if not agent_type:
            raise HTTPException(status_code=400, detail="Node has no agent type")
        
        # 4. Collect inputs from globals_schema based on 'reads'
        globals_schema = G.graph.get("globals_schema", {})
        reads = node_data.get("reads", [])
        inputs = {key: globals_schema.get(key) for key in reads if key in globals_schema}
        
        # 5. Build the input payload helper
        def build_agent_input(instruction=None, previous_output=None, iteration_context=None):
            # Determine base values
            prompt_to_use = instruction or node_data.get("agent_prompt", node_data.get("description", ""))
            query_to_use = G.graph.get("original_query", "")

            # Apply overrides from request if present
            if request and request.input:
                if agent_type == "PlannerAgent":
                    query_to_use = request.input
                else:
                    # For downstream agents, the input overrides the instruction/goal
                    prompt_to_use = request.input

            payload = {
                "step_id": node_id,
                "agent_prompt": prompt_to_use,
                "reads": reads,
                "writes": node_data.get("writes", []),
                "inputs": inputs,
                "original_query": query_to_use,
                "session_context": {
                    "session_id": run_id,
                    "created_at": G.graph.get("created_at", ""),
                    "file_manifest": G.graph.get("file_manifest", []),
                },
                **({"previous_output": previous_output} if previous_output else {}),
                **({"iteration_context": iteration_context} if iteration_context else {})
            }
             # Formatter-specific additions
            if agent_type == "FormatterAgent":
                global_data = G.graph.get('globals_schema', {}).copy()
                print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è DEBUG FORMATTER: run_id={run_id}")
                print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è DEBUG FORMATTER: file={found_file}")
                print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è DEBUG FORMATTER: globals_keys={list(global_data.keys())}")
                if 'formatted_report_T010' in global_data:
                    print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è DEBUG FORMATTER: FOUND STALE KEY 'formatted_report_T010'!")
                payload["all_globals_schema"] = global_data
            return payload

        # 6. Execute with ReAct Loop (Max 15 turns)
        from agents.base_agent import AgentRunner
        from memory.context import ExecutionContextManager
        
        agent_runner = AgentRunner(multi_mcp)
        temp_context = ExecutionContextManager.__new__(ExecutionContextManager)
        temp_context.plan_graph = G
        temp_context.multi_mcp = multi_mcp

        max_turns = 15
        current_input = build_agent_input()
        iterations_data = []
        final_output = {}
        final_execution_result = None

        for turn in range(1, max_turns + 1):
            print(f"üîÑ Test Mode: {agent_type} Iteration {turn}/{max_turns}")
            
            # Run Agent
            result = await agent_runner.run_agent(agent_type, current_input)
            
            if not result["success"]:
                return {
                    "status": "error",
                    "error": result.get("error", "Agent execution failed"),
                    "node_id": node_id,
                    "agent_type": agent_type
                }
            
            output = result["output"]
            final_output = output # Update final output
            iterations_data.append({"iteration": turn, "output": output})
            
            # 1. Check for 'call_tool' (ReAct)
            if output.get("call_tool"):
                tool_call = output["call_tool"]
                tool_name = tool_call.get("name")
                tool_args = tool_call.get("arguments", {})
                
                print(f"üõ†Ô∏è Test Mode: Executing Tool: {tool_name}")
                
                try:
                    # Execute tool via MultiMCP
                    tool_result = await multi_mcp.route_tool_call(tool_name, tool_args)
                    
                    # Serialize result content
                    if isinstance(tool_result.content, list):
                        result_str = "\n".join([str(item.text) for item in tool_result.content if hasattr(item, "text")])
                    else:
                        result_str = str(tool_result.content)

                    # Save result to history
                    iterations_data[-1]["tool_result"] = result_str
                    
                    # Prepare input for next iteration
                    instruction = output.get("thought", "Use the tool result to generate the final output.")
                    if turn == max_turns - 1:
                         instruction += " \n\n‚ö†Ô∏è WARNING: This is your FINAL turn. You MUST provide the final 'output' now. Do not call any more tools. Summarize what you have."

                    current_input = build_agent_input(
                        instruction=instruction,
                        previous_output=output,
                        iteration_context={"tool_result": result_str}
                    )
                    continue # Loop to next turn

                except Exception as e:
                    print(f"Test Mode: Tool Execution Failed: {e}")
                    current_input = build_agent_input(
                        instruction="The tool execution failed. Try a different approach or tool.",
                        previous_output=output,
                        iteration_context={"tool_result": f"Error: {str(e)}"}
                    )
                    continue

            # 2. Check for call_self (Legacy/Advanced recursion)
            elif output.get("call_self"):
                # Handle code execution if needed
                if temp_context._has_executable_code(output):
                     # Pass 'inputs' as overrides so variables from prev iterations (like ipl_urls_1A) are available
                    execution_result = await temp_context._auto_execute_code(node_id, output, input_overrides=inputs)
                    final_execution_result = execution_result
                    
                    # Save result to history
                    iterations_data[-1]["execution_result"] = execution_result

                    if execution_result.get("status") == "success":
                        execution_data = execution_result.get("result", {})
                        inputs = {**inputs, **execution_data}  # Update inputs for next iteration
                
                # Prepare input for next iteration
                current_input = build_agent_input(
                    instruction=output.get("next_instruction", "Continue the task"),
                    previous_output=output,
                    iteration_context=output.get("iteration_context", {})
                )
                continue

            # 3. Success (No tool call, just output)
            else:
                 # Execute code if present (Final Iteration)
                if temp_context._has_executable_code(output):
                     # Pass 'inputs' as overrides here too
                    final_execution_result = await temp_context._auto_execute_code(node_id, output, input_overrides=inputs)
                    iterations_data[-1]["execution_result"] = final_execution_result
                    if final_execution_result:
                         final_output = temp_context._merge_execution_results(output, final_execution_result)
                break # Exit loop
        
        # 8. Get the original output for comparison
        original_output = node_data.get("output", {})
        
        # Ensure final_execution_result is passed even if loop broke early
        if not final_execution_result and iterations_data:
             final_execution_result = iterations_data[-1].get("execution_result")

        return {
            "status": "success",
            "node_id": node_id,
            "agent_type": agent_type,
            "original_output": original_output,
            "test_output": final_output,
            "execution_result": final_execution_result,
            "inputs_used": inputs,
            "iterations": iterations_data # Optional: Pass full iterations if needed by UI
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/runs/{run_id}/agent/{node_id}/save")
async def save_agent_test(run_id: str, node_id: str, request: Request):
    """
    Save test results back to the session file.
    - Updates the node's output
    - Updates globals_schema with new writes
    """
    try:
        body = await request.json()
        new_output = body.get("output")
        
        if not new_output:
            raise HTTPException(status_code=400, detail="Missing 'output' in request body")
        
        # 1. Find the session file
        summaries_dir = PROJECT_ROOT / "memory" / "session_summaries_index"
        found_file = None
        for path in summaries_dir.rglob(f"session_{run_id}.json"):
            found_file = path
            break
        
        if not found_file:
            raise HTTPException(status_code=404, detail="Session not found")
        
        # 2. Load and update session
        import networkx as nx
        session_data = json.loads(found_file.read_text())
        if "edges" in session_data:
            G = nx.node_link_graph(session_data, edges="edges")
        elif "links" in session_data:
            G = nx.node_link_graph(session_data, edges="links")
        elif "link" in session_data:
            G = nx.node_link_graph(session_data, edges="link")
        else:
            session_data["edges"] = []
            G = nx.node_link_graph(session_data, edges="edges")
        
        if node_id not in G.nodes:
            raise HTTPException(status_code=404, detail=f"Node {node_id} not found")

        # 2.5 SPECIAL HANDLING: PlannerAgent Graph Update
        # If this is a PlannerAgent (has plan_graph in output), we must REBUILD the graph structure.
        if "plan_graph" in new_output:
            print(f"üîÑ Planner Update Detected for {node_id}. Rebuilding graph...")
            plan_graph = new_output["plan_graph"]
            
            # 1. Keep crucial nodes (ROOT and the Planner/Query node itself)
            # We assume node_id is the Planner node. 
            nodes_to_keep = ["ROOT", node_id] 
            
            # 2. Identify nodes to remove (all existing nodes except kept ones)
            nodes_to_remove = [n for n in G.nodes if n not in nodes_to_keep]
            for n in nodes_to_remove:
                G.remove_node(n)
                
            # 3. Add NEW nodes from plan
            # plan_graph['nodes'] is a list of dicts
            new_nodes = plan_graph.get("nodes", [])
            for n_data in new_nodes:
                nid = n_data["id"] 
                # Ensure we don't overwrite the planner if it's in the list for some reason (unlikely but safe)
                if nid not in G.nodes:
                    G.add_node(nid, **n_data)
                    # Initialize status for new nodes
                    G.nodes[nid]["status"] = "idle"
            
            # 4. Add NEW edges from plan
            # plan_graph['edges'] or 'links'
            new_edges = plan_graph.get("edges", plan_graph.get("links", []))
            
            # clear existing edges? We already removed nodes, so connected edges are gone.
            # But we need to ensure ROOT -> Planner connection exists if not implicitly handled.
            # In our schema, ROOT->Query (Planner). 
            # The plan_graph usually defines edges from "ROOT" to the first new node.
            # We must REMAP "ROOT" in the plan to be "node_id" (The Planner Node) 
            # so that the flow is ROOT -> Planner -> FirstNode
            
            for edge in new_edges:
                src = edge["source"]
                tgt = edge["target"]
                
                # REMAP ROOT -> Current Planner Node
                if src == "ROOT":
                    src = node_id
                    
                G.add_edge(src, tgt)
                
            # Ensure ROOT is connected to Planner (node_id)
            if not G.has_edge("ROOT", node_id):
                G.add_edge("ROOT", node_id)
                
            print(f"‚úÖ Graph Rebuilt. Nodes: {len(G.nodes)}, Edges: {len(G.edges)}")

        node_data = G.nodes[node_id]
        writes = node_data.get("writes", [])
        
        # 3. Update node output
        node_data["output"] = new_output
        node_data["last_tested"] = datetime.now().isoformat()
        node_data["status"] = "completed"
        
        # 4. Update globals_schema with execution results if available
        # 4. Update globals_schema
        # CRITICAL FIX: Prioritize the 'merged' output (new_output) which contains the actual results
        # Execution result is less reliable as it might be raw or unmerged
        
        globals_schema = G.graph.get("globals_schema", {})
        
        # 1. Try extracting from new_output (which is test_output from frontend = merged result)
        if isinstance(new_output, dict):
             for key in writes:
                if key in new_output:
                     # Validate it's not just an empty placeholder if possible, but trust the save
                     val = new_output[key]
                     # If it's a list and not empty, or dict and not empty, update
                     if val or val == 0 or val is False: 
                         globals_schema[key] = val
                         
        # 2. Fallback to execution_result only if new_output didn't have it
        exec_result = body.get("execution_result")
        if exec_result and isinstance(exec_result, dict):
             result_data = exec_result.get("result", exec_result) # Handle {status:..., result:...} or direct
             
             for key in writes:
                 if key not in globals_schema or not globals_schema[key]: # Only if missing/empty
                     if isinstance(result_data, dict) and key in result_data:
                         globals_schema[key] = result_data[key]
        
        G.graph["globals_schema"] = globals_schema
        
        # 5. Update iterations array with execution_result if provided
        execution_result = body.get("execution_result")
        if execution_result:
            # Check if iterations exist, if not create a default one
            if not node_data.get("iterations"):
                 node_data["iterations"] = []
            
            iterations = node_data["iterations"]
            
            if iterations:
                # Update the last iteration with the new execution result
                iterations[-1]["execution_result"] = execution_result
            else:
                # Create a pseudo-iteration if none exist (e.g. single-shot agents)
                iterations.append({
                    "iteration": 1, 
                    "output": new_output,
                    "execution_result": execution_result
                })

        # 5.5. Cascading Invalidation: Mark downstream nodes as 'stale'
        # This gives visual feedback (muted opacity) in the frontend that these nodes need re-running
        try:
             descendants = nx.descendants(G, node_id)
             for desc_id in descendants:
                 if desc_id in G.nodes:
                     # Only mark as stale if they were previously completed or failed
                     # If they are 'pending', they stay pending.
                     current_status = G.nodes[desc_id].get('status')
                     if current_status in ['completed', 'failed', 'running']:
                        G.nodes[desc_id]['status'] = 'stale'
        except Exception as e:
             print(f"Warning: Failed to invalidate downstream nodes: {e}")
        
        # 6. Save back to file
        # Use edges="edges" to match our expected format (not default "link")
        graph_data = nx.node_link_data(G, edges="edges")
        with open(found_file, 'w', encoding='utf-8') as f:
            json.dump(graph_data, f, indent=2, default=str, ensure_ascii=False)
        
        # 7. AUTO-SAVE TO NOTES (for FormatterAgent "Run Again" saves)
        agent_type = G.nodes[node_id].get("agent", "")
        if agent_type == "FormatterAgent" and isinstance(new_output, dict):
            try:
                import re
                notes_dir = PROJECT_ROOT / "data" / "Notes" / "Arcturus"
                notes_dir.mkdir(parents=True, exist_ok=True)
                
                def sanitize_filename(title):
                    title = re.sub(r'[\\/*?:"<>|#]', "", title)
                    title = title.replace("\n", " ").strip()
                    return title[:60].strip()

                def extract_title(content):
                    match = re.search(r'^#+\s+(.+)$', content, re.MULTILINE)
                    if match:
                        return match.group(1).strip()
                    lines = [l.strip() for l in content.split('\n') if l.strip()]
                    return lines[0] if lines else "Untitled Report"
                
                markdown = new_output.get("markdown_report")
                if not markdown:
                    for k, v in new_output.items():
                        if k.startswith("formatted_report") and isinstance(v, str):
                            markdown = v
                            break
                
                if markdown and len(markdown) > 100:
                    title = extract_title(markdown)
                    filename = sanitize_filename(title) + ".md"
                    target_path = notes_dir / filename
                    
                    # Write or overwrite (Run Again means user explicitly wants new version)
                    with open(target_path, 'w', encoding='utf-8') as f:
                        f.write(markdown)
                    print(f"‚úÖ Auto-Saved (Run Again) to Notes: {filename}")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to auto-save to Notes: {e}")
        
        return {
            "status": "success",
            "node_id": node_id,
            "message": "Test results saved to session"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
