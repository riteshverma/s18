{
    "ollama": {
        "base_url": "http://127.0.0.1:11434",
        "timeout": 300
    },
    "models": {
        "embedding": "nomic-embed-text",
        "semantic_chunking": "gemma3:4b",
        "image_captioning": "gemma3:4b",
        "memory_extraction": "qwen3-vl:8b",
        "insights_provider": "ollama"
    },
    "rag": {
        "chunk_size": 256,
        "chunk_overlap": 40,
        "max_chunk_length": 512,
        "semantic_word_limit": 1024,
        "top_k": 3
    },
    "agent": {
        "model_provider": "gemini",
        "default_model": "gemini-2.5-flash",
        "overrides": {
            "TestAgent": {
                "model_provider": "ollama",
                "model": "gemma3:4b"
            }
        },
        "max_steps": 3,
        "max_lifelines_per_step": 3,
        "planning_mode": "conservative",
        "rate_limit_interval": 4.5
    },
    "remme": {
        "extraction_prompt": "You are a Contextual Memory Management AI.\nYour job is to update the \"Memory Vault\" based on the latest conversation.\n\nRULES:\n1. ANTI-FRAGMENTATION: NEVER split related items into separate facts. Merge them into ONE rich, coherent memory entry.\n2. NO REDUNDANCY: If info is already captured, do nothing unless you have NEW details (use \"update\").\n3. CONTEXTUAL HUBS: Prefer a single \"Hub\" memory over separate atomic facts.\n4. NO NEGATIVE FACTS: NEVER store \"not found\" or \"missing\" info.\n5. NO META-LOGS: Do not store internal reasoning or agent traces.\n6. HIGH SALIENCE ONLY: Focus on project decisions, user preferences, architectural details.\n7. ACTIONS: \"add\" for new facts, \"update\" to expand existing, \"delete\" if proven false."
    },
    "gemini": {
        "api_key_env": "GEMINI_API_KEY"
    }
}